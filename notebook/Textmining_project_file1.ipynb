{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Textmining_project_file1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPxAs1o6M-6t",
        "outputId": "95e6e6b7-fd6d-4a6b-8366-693bf842b337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNA1COMdOAKt",
        "outputId": "891d21a2-a267-49d9-d0d9-1c78d3e5ffb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to clean the text data \n",
        "\n",
        "def cleantextdata(inputlist): \n",
        "  '''\n",
        "  Inputs: \n",
        "    inputlist: the original blog text\n",
        "\n",
        "  Outputs:\n",
        "    cleaned text: text data which will be used later for morphological process \n",
        "  '''\n",
        "  # to split the text data \n",
        "  splitlist = inputlist.split() \n",
        "  # to create an empty list for storing new text \n",
        "  newlist = [ ]\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  # lemmatising function\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for word in splitlist: \n",
        "    if word not in string.punctuation: \n",
        "      if word not in stopwords: \n",
        "        newlist.append(word) \n",
        "  newlist = ' '.join(newlist)\n",
        "  doc = nlp(newlist)\n",
        "  list2 = []\n",
        "  for token in doc:\n",
        "    list2.append(token.lemma_)\n",
        "  newlist = ' '.join(map(str, list2))\n",
        "  return newlist"
      ],
      "metadata": {
        "id": "MEVfJQoLOuYN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to cancatinate two files into one\n",
        "trainingdata = pd.read_csv('/content/drive/MyDrive/text_mining_project/naturalscience/train.csv')\n",
        "testdata = pd.read_csv('/content/drive/MyDrive/text_mining_project/naturalscience/test.csv')\n",
        "\n",
        "frames = [trainingdata, testdata ]\n",
        "\n",
        "sciecedf = pd.concat(frames)\n",
        "# to check the shape of the dataframe\n",
        "sciecedf['cleaned_Comments'] = sciecedf['Comment'].apply(lambda x: cleantextdata(x))\n",
        "\n",
        "sciecedf.head()"
      ],
      "metadata": {
        "id": "_-yO7MlGOVuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sciecedf.to_csv('/content/drive/MyDrive/text_mining_project/naturalscience/cleaned_text.csv')"
      ],
      "metadata": {
        "id": "BgNRfcbSYEP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RWYy564pVA9I",
        "outputId": "557a9176-5ee3-4bf2-b825-1356b51ebe76"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'there be bee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}